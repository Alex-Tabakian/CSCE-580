1. What do the accuracy and loss curves tell you about the fine-tuning process?

Accuracy and loss curves show how well a model learns. A rise in validation accuracy with a decrease in validation loss indicates correct learning. If validation loss rises while training loss drops, it suggests overfitting.

2. How does the fine-tuned DistilBERT model compare to the classical ML model? What advantages or limitations do transformers present over classical algorithms?

The DistilBERT model achieves better accuracy than the classical ML model. Transformers can capture word context while classical models only rely on word frequency.

3. What insights can you draw from the confusion matrix? Are there any patterns in the
misclassifications?

The confusion matrix for my TF-IDf shows that the majority of the predictions are correctly clasified and that there is little bias for false positives and false negatives. 
The confusion model for GPT-2 show that there is a bias for false positives as the model predicts that the most of the movies are positive regardless if it is positive or actually negative.


4. Why might the fine-tuned model outperform the base model?

The Fine tuned model is able to adapt to the DistilBERT weights. Because of this, it can learn sentiment.

5. Which model would you recommend for deployment in a real-world scenario, and why?
Consider both performance and efficiency in your answer.

I would use the fine-tuned DistilBERT model because it is the most accurate while also being more effiecient than the GTP2 and TF-IDF model.

